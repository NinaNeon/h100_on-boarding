# h100_on-boarding

```bash
ssh -J caijingnina-gmai-11e66a@jb1.frontier.nvidia.com:2222 caijingnina-gmai-11e66a@4.tun.tp1-cluster.nvidia.com
```


```bash
ssh -i ~/.ssh/id_ed25519 \
  -o "ProxyCommand=ssh -i ~/.ssh/id_ed25519 -W %h:%p caijingnina-gmai-11e66a@jb1.frontier.nvidia.com -p 2222" \
  caijingnina-gmai-11e66a@4.tun.tp1-cluster.nvidia.com
```

```bash
caijingnina-gmai-11e66a@Tunnel3-TRDC-02:~$ srun -N 1 -p NTU_Shared --gres=gpu:1 --pty /bin/bash
srun: job 5409 queued and waiting for resources
srun: job 5409 has been allocated resources
groups: cannot find name for group ID 1003
caijingnina-gmai-11e66a@cnode4-009:~$
```
```bash
caijingnina-gmai-11e66a@cnode4-009:~$ sbatch gpu.sh
sbatch: error: Unable to open file gpu.sh
caijingnina-gmai-11e66a@cnode4-009:~$ nano gpu.sh
caijingnina-gmai-11e66a@cnode4-009:~$ sbatch gpu.sh
Submitted batch job 5410
caijingnina-gmai-11e66a@cnode4-009:~$ squeue -u $USER
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
              5409 NTU_Share     bash caijingn  R       3:02      1 cnode4-009
caijingnina-gmai-11e66a@cnode4-009:~$ cat slurm-<jobID>.out
bash: jobID: No such file or directory
caijingnina-gmai-11e66a@cnode4-009:~$
```

âœ… å®Œç¾ï¼ä½ å·²ç»ç¡®è®¤æ‹¿åˆ°ä¸€å¼  **H100 80GB HBM3**ï¼Œç°åœ¨åœ¨ **è®¡ç®—èŠ‚ç‚¹ cnode4-009** ä¸Šï¼ŒGPU ç©ºé—²å¯ä»¥ä½¿ç”¨ã€‚

ä½ è·‘ `sbatch gpu.sh` æŠ¥é”™æ˜¯å› ä¸ºå½“å‰ç›®å½•ä¸‹æ²¡æœ‰ `gpu.sh` æ–‡ä»¶ã€‚
è¦ç”¨ `sbatch` æäº¤ä»»åŠ¡ï¼Œä½ éœ€è¦å…ˆå‡†å¤‡ä¸€ä¸ª **Slurm Job Script**ã€‚

---

### æœ€å°åŒ–èŒƒä¾‹ï¼šgpu.sh

åœ¨ `cnode4-009` ä¸Šè¿è¡Œï¼š

```bash
nano gpu.sh
```

ç²˜è´´ä»¥ä¸‹å†…å®¹ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ï¼š

```bash
#!/bin/bash
#SBATCH -J test_gpu          # ä»»åŠ¡åç§°
#SBATCH -p NTU_Shared        # åˆ†åŒº
#SBATCH -N 1                 # èŠ‚ç‚¹æ•°
#SBATCH --gres=gpu:1         # è¯·æ±‚ 1 å¼  GPU
#SBATCH -t 00:30:00          # æœ€é•¿è¿è¡Œæ—¶é—´ (30 åˆ†é’Ÿ)
#SBATCH -o slurm-%j.out      # æ ‡å‡†è¾“å‡º (%j = job ID)

# æ‰“å°èŠ‚ç‚¹ä¿¡æ¯å’Œ GPU çŠ¶æ€
hostname
nvidia-smi

# å¦‚æœè¦è·‘ Python ç¨‹åºï¼Œè®°å¾—åŠ è½½æ¨¡å—/conda
# module load anaconda3
# conda activate myenv
# python train.py
```

ä¿å­˜åï¼ˆ`Ctrl+O` â†’ `Enter` â†’ `Ctrl+X`ï¼‰ï¼Œå†æäº¤ï¼š

```bash
sbatch gpu.sh
```

æŸ¥çœ‹é˜Ÿåˆ—çŠ¶æ€ï¼š

```bash
squeue -u $USER
```

æŸ¥çœ‹è¾“å‡ºï¼š

```bash
cat slurm-<jobID>.out
```

---

ğŸ‘Œ ä½ ç°åœ¨å·²ç»æˆåŠŸæäº¤äº† **Job 5410**ï¼š

```bash
Submitted batch job 5410
```

è¿™æ—¶å€™è¾“å‡ºæ–‡ä»¶çš„åå­—ä¼šæ˜¯

```
slurm-5410.out
```

æ‰€ä»¥ä½ éœ€è¦è¿™æ ·æŸ¥çœ‹ï¼š

```bash
cat slurm-5410.out
```

---

### å°æŠ€å·§

1. å¦‚æœä½ ä»¥åä¸æƒ³æ‰‹åŠ¨æ”¹ `<jobID>`ï¼Œå¯ä»¥ç›´æ¥ï¼š

   ```bash
   ls slurm-*.out
   ```

   å°±èƒ½çœ‹åˆ°æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ã€‚

2. å¦‚æœ job è¿˜æ²¡è·‘å®Œï¼Œæ–‡ä»¶å¯èƒ½è¿˜æ²¡ç”Ÿæˆæˆ–å†…å®¹ä¸å…¨ã€‚
   å¯ä»¥ä¸€è¾¹ç­‰ä¸€è¾¹çœ‹ï¼š

   ```bash
   tail -f slurm-5410.out
   ```

---

è¦ä¸è¦æˆ‘å¸®ä½ æ”¹ `gpu.sh`ï¼ŒåŠ ä¸Šä¸€æ®µ **Python (PyTorch CUDA æµ‹è¯•ä»£ç )**ï¼Ÿè¿™æ ·è·‘å®Œåè¾“å‡ºæ–‡ä»¶é‡Œä¼šç›´æ¥æ˜¾ç¤º GPU èƒ½ä¸èƒ½è¢« PyTorch ç”¨ã€‚

