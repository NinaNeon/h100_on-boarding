# h100_on-boarding


https://hackmd.io/@MasonWu/r1p9Hb88gg

```bash
ssh -J caijingnina-gmai-11e66a@jb1.frontier.nvidia.com:2222 caijingnina-gmai-11e66a@4.tun.tp1-cluster.nvidia.com
```


```bash
ssh -i ~/.ssh/id_ed25519 \
  -o "ProxyCommand=ssh -i ~/.ssh/id_ed25519 -W %h:%p caijingnina-gmai-11e66a@jb1.frontier.nvidia.com -p 2222" \
  caijingnina-gmai-11e66a@4.tun.tp1-cluster.nvidia.com
```

```bash
caijingnina-gmai-11e66a@Tunnel3-TRDC-02:~$ srun -N 1 -p NTU_Shared --gres=gpu:1 --pty /bin/bash
srun: job 5409 queued and waiting for resources
srun: job 5409 has been allocated resources
groups: cannot find name for group ID 1003
caijingnina-gmai-11e66a@cnode4-009:~$
```
```bash
caijingnina-gmai-11e66a@cnode4-009:~$ sbatch gpu.sh
sbatch: error: Unable to open file gpu.sh
caijingnina-gmai-11e66a@cnode4-009:~$ nano gpu.sh
caijingnina-gmai-11e66a@cnode4-009:~$ sbatch gpu.sh
Submitted batch job 5410
caijingnina-gmai-11e66a@cnode4-009:~$ squeue -u $USER
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
              5409 NTU_Share     bash caijingn  R       3:02      1 cnode4-009
caijingnina-gmai-11e66a@cnode4-009:~$ cat slurm-<jobID>.out
bash: jobID: No such file or directory
caijingnina-gmai-11e66a@cnode4-009:~$
```
é€™ä¸€æ®µè¼¸å‡ºé¡¯ç¤ºï¼šä½ ç›®å‰ç™»å…¥çš„ç¯€é» **ï¼ˆTunnel3-TRDC-02ï¼‰ä¸¦ä¸æ˜¯è¨ˆç®—ç¯€é»**ï¼Œè€Œæ˜¯ç™»å…¥ç”¨çš„ã€Œå…¥å£ç¯€é»ï¼ˆlogin nodeï¼‰ã€æˆ–ã€Œtunnel nodeã€â”€â”€ å®ƒæ²’æœ‰å®‰è£ Slurm åŸ·è¡Œå·¥å…·ï¼ˆ`srun`, `sbatch`ï¼‰ï¼Œæ‰€ä»¥ä½ ä¸èƒ½åœ¨é€™è£¡ç›´æ¥è·‘ GPU ä»»å‹™ã€‚

---

âœ… **è§£é‡‹ä¸€ä¸‹ï¼š**

* `Tunnel3-TRDC-02` æ˜¯ã€Œå…¥å£æ©Ÿã€ï¼ˆlogin node / tunnel nodeï¼‰
* `cnode4-009` æ‰æ˜¯ä½ å‰›å‰›åˆ†é…åˆ°çš„ã€Œé‹ç®—ç¯€é»ï¼ˆcompute nodeï¼‰ã€
* Slurm å‘½ä»¤ (`srun`, `sbatch`, `sinfo`, `squeue`) åªèƒ½åœ¨æœ‰å®‰è£ Slurm çš„ç’°å¢ƒä½¿ç”¨ã€‚é€šå¸¸æœƒåœ¨ä¸»é›†ç¾¤ç¯€é»ï¼ˆä¾‹å¦‚ `jb1.frontier.nvidia.com` ä¹‹å¾Œé€£é€²çš„ä¸»æ§ç¯€é»ï¼‰åŸ·è¡Œã€‚

---

### âœ… æ­£ç¢ºçš„åšæ³•ï¼š

è«‹ä½ é‡æ–°é€£ç·šåˆ° **ä¸»æ§ç¯€é»** æˆ– **å…·å‚™ Slurm çš„ç¯€é»**ï¼Œç„¶å¾Œå†ä½¿ç”¨ `srun`ã€‚

ä½ å¯ä»¥ç”¨ä¸‹é¢é€™å€‹å‘½ä»¤é€²å…¥æ­£ç¢ºç’°å¢ƒï¼ˆä½ ä¹‹å‰æˆåŠŸéï¼‰ï¼š

```bash
ssh -i ~/.ssh/id_ed25519 \
  -o "ProxyCommand=ssh -i ~/.ssh/id_ed25519 -W %h:%p caijingnina-gmai-11e66a@jb1.frontier.nvidia.com -p 2222" \
  caijingnina-gmai-11e66a@4.tun.tp1-cluster.nvidia.com
```

é€²å»å¾Œä½ æœƒçœ‹åˆ°æç¤ºå­—æ¨£åƒæ˜¯ï¼š

```
Welcome to Base Command Manager 10.0
```

ä¸¦ä¸”å¯ä»¥ç”¨ï¼š

```bash
module load slurm
sinfo
```

é€™æ¨£æ‰å¯ä»¥è·‘ï¼š

```bash
srun -N 1 -p NTU_Shared --gres=gpu:1 --pty /bin/bash
```

---

ğŸ”¹ **é‡é»ï¼š**

* Tunnel node (`Tunnel3-TRDC-02`) âœ âŒ æ²’æœ‰ Slurmï¼Œä¸èƒ½è·‘ GPU ä»»å‹™
* Cluster node (`tp1-cluster.nvidia.com` æˆ– `cnode4-*`) âœ âœ… å¯ä»¥è·‘ Slurm æŒ‡ä»¤

---

è¦ä¸è¦æˆ‘å¹«ä½ æ•´ç†ä¸€ä»½ **å¾ç™»å…¥ â†’ è¼‰å…¥æ¨¡çµ„ â†’ å•Ÿå‹• GPU â†’ åŸ·è¡Œ PyTorch æ¸¬è©¦** çš„å®Œæ•´ã€Œä¸€éµæµç¨‹ç¯„ä¾‹ã€ï¼Ÿ
è²¼ä¸Šå°±èƒ½ç›´æ¥ä½¿ç”¨ï¼ˆå¾ç™»å…¥ç¯€é»é–‹å§‹ä¸€è·¯åŸ·è¡Œï¼‰ã€‚

âœ… å®Œç¾ï¼ä½ å·²ç»ç¡®è®¤æ‹¿åˆ°ä¸€å¼  **H100 80GB HBM3**ï¼Œç°åœ¨åœ¨ **è®¡ç®—èŠ‚ç‚¹ cnode4-009** ä¸Šï¼ŒGPU ç©ºé—²å¯ä»¥ä½¿ç”¨ã€‚

ä½ è·‘ `sbatch gpu.sh` æŠ¥é”™æ˜¯å› ä¸ºå½“å‰ç›®å½•ä¸‹æ²¡æœ‰ `gpu.sh` æ–‡ä»¶ã€‚
è¦ç”¨ `sbatch` æäº¤ä»»åŠ¡ï¼Œä½ éœ€è¦å…ˆå‡†å¤‡ä¸€ä¸ª **Slurm Job Script**ã€‚

---

### æœ€å°åŒ–èŒƒä¾‹ï¼šgpu.sh

åœ¨ `cnode4-009` ä¸Šè¿è¡Œï¼š

```bash
nano gpu.sh
```

ç²˜è´´ä»¥ä¸‹å†…å®¹ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ï¼š

```bash
#!/bin/bash
#SBATCH -J test_gpu          # ä»»åŠ¡åç§°
#SBATCH -p NTU_Shared        # åˆ†åŒº
#SBATCH -N 1                 # èŠ‚ç‚¹æ•°
#SBATCH --gres=gpu:1         # è¯·æ±‚ 1 å¼  GPU
#SBATCH -t 00:30:00          # æœ€é•¿è¿è¡Œæ—¶é—´ (30 åˆ†é’Ÿ)
#SBATCH -o slurm-%j.out      # æ ‡å‡†è¾“å‡º (%j = job ID)

# æ‰“å°èŠ‚ç‚¹ä¿¡æ¯å’Œ GPU çŠ¶æ€
hostname
nvidia-smi

# å¦‚æœè¦è·‘ Python ç¨‹åºï¼Œè®°å¾—åŠ è½½æ¨¡å—/conda
# module load anaconda3
# conda activate myenv
# python train.py
```

ä¿å­˜åï¼ˆ`Ctrl+O` â†’ `Enter` â†’ `Ctrl+X`ï¼‰ï¼Œå†æäº¤ï¼š

```bash
sbatch gpu.sh
```

æŸ¥çœ‹é˜Ÿåˆ—çŠ¶æ€ï¼š

```bash
squeue -u $USER
```

æŸ¥çœ‹è¾“å‡ºï¼š

```bash
cat slurm-<jobID>.out
```

---

ğŸ‘Œ ä½ ç°åœ¨å·²ç»æˆåŠŸæäº¤äº† **Job 5410**ï¼š

```bash
Submitted batch job 5410
```

è¿™æ—¶å€™è¾“å‡ºæ–‡ä»¶çš„åå­—ä¼šæ˜¯

```
slurm-5410.out
```

æ‰€ä»¥ä½ éœ€è¦è¿™æ ·æŸ¥çœ‹ï¼š

```bash
cat slurm-5410.out
```

---

### å°æŠ€å·§

1. å¦‚æœä½ ä»¥åä¸æƒ³æ‰‹åŠ¨æ”¹ `<jobID>`ï¼Œå¯ä»¥ç›´æ¥ï¼š

   ```bash
   ls slurm-*.out
   ```

   å°±èƒ½çœ‹åˆ°æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ã€‚

2. å¦‚æœ job è¿˜æ²¡è·‘å®Œï¼Œæ–‡ä»¶å¯èƒ½è¿˜æ²¡ç”Ÿæˆæˆ–å†…å®¹ä¸å…¨ã€‚
   å¯ä»¥ä¸€è¾¹ç­‰ä¸€è¾¹çœ‹ï¼š

   ```bash
   tail -f slurm-5410.out
   ```

---

è¦ä¸è¦æˆ‘å¸®ä½ æ”¹ `gpu.sh`ï¼ŒåŠ ä¸Šä¸€æ®µ **Python (PyTorch CUDA æµ‹è¯•ä»£ç )**ï¼Ÿè¿™æ ·è·‘å®Œåè¾“å‡ºæ–‡ä»¶é‡Œä¼šç›´æ¥æ˜¾ç¤º GPU èƒ½ä¸èƒ½è¢« PyTorch ç”¨ã€‚

