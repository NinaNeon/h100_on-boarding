# h100_on-boarding


https://hackmd.io/@MasonWu/r1p9Hb88gg

```bash
ssh -J caijingnina-gmai-11e66a@jb1.frontier.nvidia.com:2222 caijingnina-gmai-11e66a@4.tun.tp1-cluster.nvidia.com
```


```bash
ssh -i ~/.ssh/id_ed25519 \
  -o "ProxyCommand=ssh -i ~/.ssh/id_ed25519 -W %h:%p caijingnina-gmai-11e66a@jb1.frontier.nvidia.com -p 2222" \
  caijingnina-gmai-11e66a@4.tun.tp1-cluster.nvidia.com
```

```bash
caijingnina-gmai-11e66a@Tunnel3-TRDC-02:~$ srun -N 1 -p NTU_Shared --gres=gpu:1 --pty /bin/bash
srun: job 5409 queued and waiting for resources
srun: job 5409 has been allocated resources
groups: cannot find name for group ID 1003
caijingnina-gmai-11e66a@cnode4-009:~$
```
```bash
caijingnina-gmai-11e66a@cnode4-009:~$ sbatch gpu.sh
sbatch: error: Unable to open file gpu.sh
caijingnina-gmai-11e66a@cnode4-009:~$ nano gpu.sh
caijingnina-gmai-11e66a@cnode4-009:~$ sbatch gpu.sh
Submitted batch job 5410
caijingnina-gmai-11e66a@cnode4-009:~$ squeue -u $USER
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
              5409 NTU_Share     bash caijingn  R       3:02      1 cnode4-009
caijingnina-gmai-11e66a@cnode4-009:~$ cat slurm-<jobID>.out
bash: jobID: No such file or directory
caijingnina-gmai-11e66a@cnode4-009:~$
```
é€™ä¸€æ®µè¼¸å‡ºé¡¯ç¤ºï¼šä½ ç›®å‰ç™»å…¥çš„ç¯€é» **ï¼ˆTunnel3-TRDC-02ï¼‰ä¸¦ä¸æ˜¯è¨ˆç®—ç¯€é»**ï¼Œè€Œæ˜¯ç™»å…¥ç”¨çš„ã€Œå…¥å£ç¯€é»ï¼ˆlogin nodeï¼‰ã€æˆ–ã€Œtunnel nodeã€â”€â”€ å®ƒæ²’æœ‰å®‰è£ Slurm åŸ·è¡Œå·¥å…·ï¼ˆ`srun`, `sbatch`ï¼‰ï¼Œæ‰€ä»¥ä½ ä¸èƒ½åœ¨é€™è£¡ç›´æ¥è·‘ GPU ä»»å‹™ã€‚

---

âœ… **è§£é‡‹ä¸€ä¸‹ï¼š**

* `Tunnel3-TRDC-02` æ˜¯ã€Œå…¥å£æ©Ÿã€ï¼ˆlogin node / tunnel nodeï¼‰
* `cnode4-009` æ‰æ˜¯ä½ å‰›å‰›åˆ†é…åˆ°çš„ã€Œé‹ç®—ç¯€é»ï¼ˆcompute nodeï¼‰ã€
* Slurm å‘½ä»¤ (`srun`, `sbatch`, `sinfo`, `squeue`) åªèƒ½åœ¨æœ‰å®‰è£ Slurm çš„ç’°å¢ƒä½¿ç”¨ã€‚é€šå¸¸æœƒåœ¨ä¸»é›†ç¾¤ç¯€é»ï¼ˆä¾‹å¦‚ `jb1.frontier.nvidia.com` ä¹‹å¾Œé€£é€²çš„ä¸»æ§ç¯€é»ï¼‰åŸ·è¡Œã€‚

---

### âœ… æ­£ç¢ºçš„åšæ³•ï¼š

è«‹ä½ é‡æ–°é€£ç·šåˆ° **ä¸»æ§ç¯€é»** æˆ– **å…·å‚™ Slurm çš„ç¯€é»**ï¼Œç„¶å¾Œå†ä½¿ç”¨ `srun`ã€‚

ä½ å¯ä»¥ç”¨ä¸‹é¢é€™å€‹å‘½ä»¤é€²å…¥æ­£ç¢ºç’°å¢ƒï¼ˆä½ ä¹‹å‰æˆåŠŸéï¼‰ï¼š

```bash
ssh -i ~/.ssh/id_ed25519 \
  -o "ProxyCommand=ssh -i ~/.ssh/id_ed25519 -W %h:%p caijingnina-gmai-11e66a@jb1.frontier.nvidia.com -p 2222" \
  caijingnina-gmai-11e66a@4.tun.tp1-cluster.nvidia.com
```

é€²å»å¾Œä½ æœƒçœ‹åˆ°æç¤ºå­—æ¨£åƒæ˜¯ï¼š

```
Welcome to Base Command Manager 10.0
```

ä¸¦ä¸”å¯ä»¥ç”¨ï¼š

```bash
module load slurm
sinfo
```

é€™æ¨£æ‰å¯ä»¥è·‘ï¼š

```bash
srun -N 1 -p NTU_Shared --gres=gpu:1 --pty /bin/bash
```

---

ğŸ”¹ **é‡é»ï¼š**

* Tunnel node (`Tunnel3-TRDC-02`) âœ âŒ æ²’æœ‰ Slurmï¼Œä¸èƒ½è·‘ GPU ä»»å‹™
* Cluster node (`tp1-cluster.nvidia.com` æˆ– `cnode4-*`) âœ âœ… å¯ä»¥è·‘ Slurm æŒ‡ä»¤

---

è¦ä¸è¦æˆ‘å¹«ä½ æ•´ç†ä¸€ä»½ **å¾ç™»å…¥ â†’ è¼‰å…¥æ¨¡çµ„ â†’ å•Ÿå‹• GPU â†’ åŸ·è¡Œ PyTorch æ¸¬è©¦** çš„å®Œæ•´ã€Œä¸€éµæµç¨‹ç¯„ä¾‹ã€ï¼Ÿ
è²¼ä¸Šå°±èƒ½ç›´æ¥ä½¿ç”¨ï¼ˆå¾ç™»å…¥ç¯€é»é–‹å§‹ä¸€è·¯åŸ·è¡Œï¼‰ã€‚

âœ… å®Œç¾ï¼ä½ å·²ç»ç¡®è®¤æ‹¿åˆ°ä¸€å¼  **H100 80GB HBM3**ï¼Œç°åœ¨åœ¨ **è®¡ç®—èŠ‚ç‚¹ cnode4-009** ä¸Šï¼ŒGPU ç©ºé—²å¯ä»¥ä½¿ç”¨ã€‚

ä½ è·‘ `sbatch gpu.sh` æŠ¥é”™æ˜¯å› ä¸ºå½“å‰ç›®å½•ä¸‹æ²¡æœ‰ `gpu.sh` æ–‡ä»¶ã€‚
è¦ç”¨ `sbatch` æäº¤ä»»åŠ¡ï¼Œä½ éœ€è¦å…ˆå‡†å¤‡ä¸€ä¸ª **Slurm Job Script**ã€‚

---

### æœ€å°åŒ–èŒƒä¾‹ï¼šgpu.sh

åœ¨ `cnode4-009` ä¸Šè¿è¡Œï¼š

```bash
nano gpu.sh
```

ç²˜è´´ä»¥ä¸‹å†…å®¹ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ï¼š

```bash
#!/bin/bash
#SBATCH -J test_gpu          # ä»»åŠ¡åç§°
#SBATCH -p NTU_Shared        # åˆ†åŒº
#SBATCH -N 1                 # èŠ‚ç‚¹æ•°
#SBATCH --gres=gpu:1         # è¯·æ±‚ 1 å¼  GPU
#SBATCH -t 00:30:00          # æœ€é•¿è¿è¡Œæ—¶é—´ (30 åˆ†é’Ÿ)
#SBATCH -o slurm-%j.out      # æ ‡å‡†è¾“å‡º (%j = job ID)

# æ‰“å°èŠ‚ç‚¹ä¿¡æ¯å’Œ GPU çŠ¶æ€
hostname
nvidia-smi

# å¦‚æœè¦è·‘ Python ç¨‹åºï¼Œè®°å¾—åŠ è½½æ¨¡å—/conda
# module load anaconda3
# conda activate myenv
# python train.py
```

ä¿å­˜åï¼ˆ`Ctrl+O` â†’ `Enter` â†’ `Ctrl+X`ï¼‰ï¼Œå†æäº¤ï¼š

```bash
sbatch gpu.sh
```

æŸ¥çœ‹é˜Ÿåˆ—çŠ¶æ€ï¼š

```bash
squeue -u $USER
```

æŸ¥çœ‹è¾“å‡ºï¼š

```bash
cat slurm-<jobID>.out
```

---

ğŸ‘Œ ä½ ç°åœ¨å·²ç»æˆåŠŸæäº¤äº† **Job 5410**ï¼š

```bash
Submitted batch job 5410
```

è¿™æ—¶å€™è¾“å‡ºæ–‡ä»¶çš„åå­—ä¼šæ˜¯

```
slurm-5410.out
```

æ‰€ä»¥ä½ éœ€è¦è¿™æ ·æŸ¥çœ‹ï¼š

```bash
cat slurm-5410.out
```

---

### å°æŠ€å·§

1. å¦‚æœä½ ä»¥åä¸æƒ³æ‰‹åŠ¨æ”¹ `<jobID>`ï¼Œå¯ä»¥ç›´æ¥ï¼š

   ```bash
   ls slurm-*.out
   ```

   å°±èƒ½çœ‹åˆ°æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ã€‚

2. å¦‚æœ job è¿˜æ²¡è·‘å®Œï¼Œæ–‡ä»¶å¯èƒ½è¿˜æ²¡ç”Ÿæˆæˆ–å†…å®¹ä¸å…¨ã€‚
   å¯ä»¥ä¸€è¾¹ç­‰ä¸€è¾¹çœ‹ï¼š

   ```bash
   tail -f slurm-5410.out
   ```

---

è¦ä¸è¦æˆ‘å¸®ä½ æ”¹ `gpu.sh`ï¼ŒåŠ ä¸Šä¸€æ®µ **Python (PyTorch CUDA æµ‹è¯•ä»£ç )**ï¼Ÿè¿™æ ·è·‘å®Œåè¾“å‡ºæ–‡ä»¶é‡Œä¼šç›´æ¥æ˜¾ç¤º GPU èƒ½ä¸èƒ½è¢« PyTorch ç”¨ã€‚
æˆ‘æœƒå¹«æ‚¨å»ºç«‹ä¸€å€‹ä½¿ç”¨ odise.yml å’Œ requirements.txt çš„è‡ªè¨‚å®¹å™¨æ˜ åƒæª”ã€‚è®“æˆ‘æ•´ç†ä¸€å€‹å®Œæ•´çš„æ­¥é©ŸæŒ‡å—ï¼š

## åœ¨ Taipei-1 ä¸Šå»ºç«‹ ODISE è‡ªè¨‚æ˜ åƒæª”

### æ­¥é©Ÿ 1ï¼šç™»å…¥é‹ç®—ç¯€é»
```bash
srun -N 1 -p defq --mpi=pmix --gres=gpu:1 --ntasks-per-node 1 --pty /bin/bash
```

### æ­¥é©Ÿ 2ï¼šä¸‹è¼‰åŸºç¤æ˜ åƒ
ç”±æ–¼æ‚¨çš„ç’°å¢ƒéœ€è¦ Python 3.9 å’Œ PyTorch 1.13.1ï¼Œå»ºè­°ä½¿ç”¨ PyTorch NGC å®¹å™¨ä½œç‚ºåŸºç¤ï¼š

```bash
# ä¸‹è¼‰ PyTorch 22.12 å®¹å™¨ (åŒ…å« PyTorch 1.13)
enroot import docker://nvcr.io#nvidia/pytorch:22.12-py3
```

é€™æœƒç”Ÿæˆé¡ä¼¼ `nvidia+pytorch+22.12-py3.sqsh` çš„æª”æ¡ˆã€‚

### æ­¥é©Ÿ 3ï¼šå»ºç«‹å®¹å™¨
```bash
enroot create --name odise_custom nvidia+pytorch+22.12-py3.sqsh
```

### æ­¥é©Ÿ 4ï¼šå•Ÿå‹•å®¹å™¨ä¸¦å®‰è£å¥—ä»¶
```bash
enroot start --root -w odise_custom /bin/bash
```

é€²å…¥å®¹å™¨å¾Œï¼ŒåŸ·è¡Œä»¥ä¸‹å®‰è£æŒ‡ä»¤ï¼š

```bash
# æ›´æ–° pip
pip install --upgrade pip

# å®‰è£ requirements.txt ä¸­çš„å¥—ä»¶
pip install absl-py==2.3.1
pip install aiohappyeyeballs==2.6.1
pip install aiohttp==3.12.15
pip install aiosignal==1.4.0
pip install antlr4-python3-runtime==4.9.3
pip install appdirs==1.4.4
pip install async-timeout==5.0.1
pip install attrs==25.3.0
pip install black==21.4b2
pip install boto3==1.40.25
pip install cloudpickle==3.1.1
pip install contourpy==1.3.0
pip install cycler==0.12.1
pip install einops==0.8.1
pip install filelock==3.19.1
pip install ftfy==6.3.1
pip install hydra-core==1.3.2
pip install omegaconf==2.3.0
pip install open-clip-torch==2.20.0
pip install opencv-python==4.11.0.86
pip install Pillow==9.5.0
pip install pycocotools==2.0.10
pip install pytorch-lightning==1.9.5
pip install PyYAML==6.0.2
pip install scipy==1.10.1
pip install transformers==4.36.2
pip install timm==0.6.11
pip install kornia==0.6.12

# å®‰è£ Detectron2
pip install 'git+https://github.com/facebookresearch/detectron2.git'

# å®‰è£å…¶ä»–é‡è¦å¥—ä»¶
pip install scikit-image==0.24.0
pip install sentencepiece==0.2.1
pip install tqdm==4.67.1
pip install termcolor==3.1.0

# é›¢é–‹å®¹å™¨
exit
```

### æ­¥é©Ÿ 5ï¼šåŒ¯å‡ºæ˜ åƒæª”
```bash
enroot export --output odise_custom.sqsh odise_custom
```

### æ­¥é©Ÿ 6ï¼šæ¸…ç†å®¹å™¨
```bash
enroot remove odise_custom
exit
```

---

## ä½¿ç”¨å»ºç«‹å¥½çš„æ˜ åƒæª”

### äº’å‹•å¼æ¨¡å¼æ¸¬è©¦
```bash
srun \
  -N 1 \
  -p defq \
  --mpi=pmix \
  --gres=gpu:1 \
  --ntasks-per-node 1 \
  --container-image ${HOME}/odise_custom.sqsh \
  --container-writable \
  --container-remap-root \
  --pty /bin/bash
```

### åŸ·è¡Œè¨“ç·´ä½œæ¥­ï¼ˆå–®ç¯€é» 8 GPUï¼‰
```bash
srun \
  -N 1 \
  -p defq \
  --mpi=pmix \
  --gres=gpu:8 \
  --ntasks-per-node 1 \
  --container-image ${HOME}/odise_custom.sqsh \
  --container-writable \
  --container-mounts /mnt/SCRATCH/:/mnt/SCRATCH \
  python your_training_script.py
```

---

## æ³¨æ„äº‹é …

1. **å¥—ä»¶ç‰ˆæœ¬è¡çª**ï¼šæ‚¨çš„ odise.yml å’Œ requirements.txt æœ‰äº›å¥—ä»¶ç‰ˆæœ¬ä¸åŒï¼ˆå¦‚ blackã€numpy ç­‰ï¼‰ï¼Œæˆ‘ä»¥ requirements.txt ç‚ºä¸»é€²è¡Œå®‰è£

2. **PyTorch ç‰ˆæœ¬**ï¼šåŸºç¤æ˜ åƒçš„ PyTorch ç‰ˆæœ¬éœ€èˆ‡æ‚¨è¦æ±‚çš„ 1.13.1 åŒ¹é…ï¼ŒNGC çš„ 22.12 ç‰ˆæœ¬åŒ…å«æ­¤ç‰ˆæœ¬

3. **æ™‚é–“è€ƒé‡**ï¼šå®Œæ•´å®‰è£æ‰€æœ‰å¥—ä»¶å¯èƒ½éœ€è¦ 10-30 åˆ†é˜ï¼Œè«‹è€å¿ƒç­‰å€™

4. **å„²å­˜ä½ç½®**ï¼šç”Ÿæˆçš„ `odise_custom.sqsh` æœƒåœ¨æ‚¨ç•¶å‰ç›®éŒ„ï¼Œå»ºè­°ç§»å‹•åˆ° `${HOME}` æˆ–å°ˆæ¡ˆç›®éŒ„

éœ€è¦æˆ‘æä¾›ä¸€å€‹è‡ªå‹•åŒ–å®‰è£è…³æœ¬å—ï¼Ÿ

```bash
caijingnina-gmai-11e66a@cnode4-005:~$ ^C
caijingnina-gmai-11e66a@cnode4-005:~$ exit
exit
srun: error: cnode4-005: task 0: Exited with exit code 130
caijingnina-gmai-11e66a@Tunnel3-TRDC-02:~$ srun \
  -N 1 \
  -p NTU_Shared \
  --mpi=pmix \
  --gres=gpu:1 \
  --ntasks-per-node=1 \
  --container-image=${HOME}/odise_custom.sqsh \
  --container-writable \
  --container-remap-root \
  --pty /bin/bash
srun: job 5440 queued and waiting for resources
srun: job 5440 has been allocated resources
root@cnode4-005:/workspace#
```
